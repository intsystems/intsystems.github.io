### О курсе

Курс посвящен современным генеративным моделям машинного обучения.
Особое внимание уделяется свойствам различных классов генеративных моделей, их взаимосвязям, теоретическим предпосылкам и методам оценки качества.
Цель курса - познакомить студента с широко используемыми передовыми методами построения порождающих моделей.

### Тематический план

1. Введение в генеративное моделирование. Постановка задачи. Задача минимизации дивергенций. Авторегрессионное моделирование (PixelCNN).
2. Модели нормализующих потоков (NF). Линейные потоки (Glow). Авторегрессионные потоки (гауссовский и обраный гауссовский поток). Слой связи (RealNVP).
3. Прямая и обратная KL дивергенции. Модели скрытых переменных. Вариационная нижняя оценка (ELBO). EM-алгоритм.
4. Амортизированный вывод. Градиенты ELBO, репараметризация. Вариационный автокодировщик (VAE). Связь нормализующих потоков и VAE. VAE с дискретным скрытым пространством.
5. Векторная квантизация, сквозной градиент (VQ-VAE). Гумбель-софтмакс трюк (DALL-E). Теорема об операции над ELBO. Оптимальное априорное распределение в VAE. Потоки в априорном распределении VAE.
6. Неявные генеративные модели без оценки правдоподобия. Модель генеративных состязательных сетей (GAN). Теорема об оптимальности GAN. Расстояние Вассерштейна.
7. GAN Вассерштейна (WGAN). Вариационная минимизация f-дивергенций. Оценивание качества неявных моделей (Inception score, FID, Precision-Recall, truncation trick).
8. Динамика Ланжевена. Методы оценивания score функции. Модели оценки score функции (NCSM). Гауссовский диффузионый процесс.
9. Denoising score matching для диффузии. Обратный гауссовский диффузионный процесс. Гауссовская диффузионная модель как VAE. ELBO для DDPM.
10. Denoising diffusion probabilistic model (DDPM): репараметризация и обзор. Denoising diffusion как score-based генеративная модель. Model guidance: classifier guidance, classifier-free guidance.
11. Нормализующие потоки непрерывного времени и neural ODE. Уравнение непрерывности для log-likelihood NF. FFJORD и оценка следа Хатчинсона. Adjoint метод для NF непрерывного времени.
12. Основы SDE. Уравнение Колмогорова-Фоккера-Планка. Probability flow ODE. Обратное SDE. Variance Preserving и Variance Exploding SDE.
13. Score-based генеративные модели через SDE. Flow matching. Conditional flow matching. Конические гауссовские пути.
14. Линейная интерполяция. Связь с диффузией и score matching. Модели скрытого пространства.

### Cамостоятельная работа

6 домашних работ, каждая содержит как теоретические задания, так и практические.

### Оценивание

Каждая домашняя работа дает 15 баллов + экзамен на 30 баллов. Финальная оценка: min(floor(#баллов/10), 10).

### Требуемые знания

Теория вероятностей, статистика, машинное обучение, основы глубокого обучения, python, pytorch, элементы байесовского вывода.
